{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnHcDyOyAHM"
      },
      "source": [
        "# 음성합성 데모\n",
        "\n",
        "이 문서는 T2T, TTS 전과정을 거쳐 표준어 텍스트로 경상도 사투리 음성 합성을 진행하는 데모입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T2T\n",
        "표준어 텍스트를 경상도 사투리 텍스트로 변환합니다."
      ],
      "metadata": {
        "id": "ytPbSE9D6oNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kss transformers"
      ],
      "metadata": {
        "id": "hl7DMsfy6bz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "EE9Act4R6eU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_text_to_dialect_text(prompt):\n",
        "    with torch.no_grad():\n",
        "        splited_sentences = kss.split_sentences(prompt)\n",
        "        generated = []\n",
        "        for sentence in splited_sentences:\n",
        "            tokens = tokenizer.encode(sentence, return_tensors=\"pt\").to(\n",
        "                device=\"cuda\", non_blocking=True\n",
        "            )\n",
        "            gen_tokens = model.generate(\n",
        "                tokens, max_length=256, repetition_penalty=1.5\n",
        "            )\n",
        "            generated.append(\n",
        "                tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
        "            )\n",
        "    return ' '.join(generated)"
      ],
      "metadata": {
        "id": "DaO2Q7I16gS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T2T_MODEL_NAME = \"dannykm/DialectKoUL2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(T2T_MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(T2T_MODEL_NAME).to(device=\"cuda\")\n",
        "_ = model.eval()\n",
        "\n",
        "prompt = input(\"텍스트를 입력하세요: \")\n",
        "dialect_text = standard_text_to_dialect_text(prompt)\n",
        "print(f\"사투리로 변환된 텍스트: {dialect_text}\")"
      ],
      "metadata": {
        "id": "Onsmm6596iVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TTS"
      ],
      "metadata": {
        "id": "Ihz0nRDyU-a7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8erClGSnzwge"
      },
      "source": [
        "### 1. 필수 라이브러리 및 함수 불러오기\n",
        "\n",
        "실행에 필요한 라이브러리 및 함수를 불러옵니다.\n",
        "\n",
        "이 과정은 약 10분 정도 소요될 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYCym6hXge2_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkWG-L13gReB"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone --depth 1 https://github.com/sce-tts/TTS.git -b sce-tts\n",
        "!git clone --depth 1 https://github.com/sce-tts/g2pK.git\n",
        "%cd /content/TTS\n",
        "!pip install -q --no-cache-dir -e .\n",
        "%cd /content/g2pK\n",
        "!pip install -q --no-cache-dir \"konlpy\" \"jamo\" \"nltk\" \"python-mecab-ko\"\n",
        "!pip install -q --no-cache-dir -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUD8SfIxSY8j"
      },
      "outputs": [],
      "source": [
        "%cd /content/g2pK\n",
        "import g2pk\n",
        "g2p = g2pk.G2p()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRU_B5TdyFWT"
      },
      "outputs": [],
      "source": [
        "!pip install pysbd coqpit unidecode pypinyin librosa==0.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt9bLLZ8I4GH"
      },
      "outputs": [],
      "source": [
        "%cd /content/TTS\n",
        "import re\n",
        "import sys\n",
        "from unicodedata import normalize\n",
        "import IPython\n",
        "\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    for c in \",;:\":\n",
        "        text = text.replace(c, \".\")\n",
        "    text = remove_duplicated_punctuations(text)\n",
        "\n",
        "    text = jamo_text(text)\n",
        "\n",
        "    text = g2p.idioms(text)\n",
        "    text = g2pk.english.convert_eng(text, g2p.cmu)\n",
        "    text = g2pk.utils.annotate(text, g2p.mecab)\n",
        "    text = g2pk.numerals.convert_num(text)\n",
        "    text = re.sub(\"/[PJEB]\", \"\", text)\n",
        "\n",
        "    text = alphabet_text(text)\n",
        "\n",
        "    # remove unreadable characters\n",
        "    text = normalize(\"NFD\", text)\n",
        "    text = \"\".join(c for c in text if c in symbols)\n",
        "    text = normalize(\"NFC\", text)\n",
        "\n",
        "    text = text.strip()\n",
        "    if len(text) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # only single punctuation\n",
        "    if text in '.!?':\n",
        "        return punctuation_text(text)\n",
        "\n",
        "    # append punctuation if there is no punctuation at the end of the text\n",
        "    if text[-1] not in '.!?':\n",
        "        text += '.'\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_duplicated_punctuations(text):\n",
        "    text = re.sub(r\"[.?!]+\\?\", \"?\", text)\n",
        "    text = re.sub(r\"[.?!]+!\", \"!\", text)\n",
        "    text = re.sub(r\"[.?!]+\\.\", \".\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_text(text):\n",
        "    text = remove_duplicated_punctuations(text)\n",
        "\n",
        "    texts = []\n",
        "    for subtext in re.findall(r'[^.!?\\n]*[.!?\\n]', text):\n",
        "        texts.append(subtext.strip())\n",
        "\n",
        "    return texts\n",
        "\n",
        "\n",
        "def alphabet_text(text):\n",
        "    text = re.sub(r\"(a|A)\", \"에이\", text)\n",
        "    text = re.sub(r\"(b|B)\", \"비\", text)\n",
        "    text = re.sub(r\"(c|C)\", \"씨\", text)\n",
        "    text = re.sub(r\"(d|D)\", \"디\", text)\n",
        "    text = re.sub(r\"(e|E)\", \"이\", text)\n",
        "    text = re.sub(r\"(f|F)\", \"에프\", text)\n",
        "    text = re.sub(r\"(g|G)\", \"쥐\", text)\n",
        "    text = re.sub(r\"(h|H)\", \"에이치\", text)\n",
        "    text = re.sub(r\"(i|I)\", \"아이\", text)\n",
        "    text = re.sub(r\"(j|J)\", \"제이\", text)\n",
        "    text = re.sub(r\"(k|K)\", \"케이\", text)\n",
        "    text = re.sub(r\"(l|L)\", \"엘\", text)\n",
        "    text = re.sub(r\"(m|M)\", \"엠\", text)\n",
        "    text = re.sub(r\"(n|N)\", \"엔\", text)\n",
        "    text = re.sub(r\"(o|O)\", \"오\", text)\n",
        "    text = re.sub(r\"(p|P)\", \"피\", text)\n",
        "    text = re.sub(r\"(q|Q)\", \"큐\", text)\n",
        "    text = re.sub(r\"(r|R)\", \"알\", text)\n",
        "    text = re.sub(r\"(s|S)\", \"에스\", text)\n",
        "    text = re.sub(r\"(t|T)\", \"티\", text)\n",
        "    text = re.sub(r\"(u|U)\", \"유\", text)\n",
        "    text = re.sub(r\"(v|V)\", \"브이\", text)\n",
        "    text = re.sub(r\"(w|W)\", \"더블유\", text)\n",
        "    text = re.sub(r\"(x|X)\", \"엑스\", text)\n",
        "    text = re.sub(r\"(y|Y)\", \"와이\", text)\n",
        "    text = re.sub(r\"(z|Z)\", \"지\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def punctuation_text(text):\n",
        "    # 문장부호\n",
        "    text = re.sub(r\"!\", \"느낌표\", text)\n",
        "    text = re.sub(r\"\\?\", \"물음표\", text)\n",
        "    text = re.sub(r\"\\.\", \"마침표\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def jamo_text(text):\n",
        "    # 기본 자모음\n",
        "    text = re.sub(r\"ㄱ\", \"기역\", text)\n",
        "    text = re.sub(r\"ㄴ\", \"니은\", text)\n",
        "    text = re.sub(r\"ㄷ\", \"디귿\", text)\n",
        "    text = re.sub(r\"ㄹ\", \"리을\", text)\n",
        "    text = re.sub(r\"ㅁ\", \"미음\", text)\n",
        "    text = re.sub(r\"ㅂ\", \"비읍\", text)\n",
        "    text = re.sub(r\"ㅅ\", \"시옷\", text)\n",
        "    text = re.sub(r\"ㅇ\", \"이응\", text)\n",
        "    text = re.sub(r\"ㅈ\", \"지읒\", text)\n",
        "    text = re.sub(r\"ㅊ\", \"치읓\", text)\n",
        "    text = re.sub(r\"ㅋ\", \"키읔\", text)\n",
        "    text = re.sub(r\"ㅌ\", \"티읕\", text)\n",
        "    text = re.sub(r\"ㅍ\", \"피읖\", text)\n",
        "    text = re.sub(r\"ㅎ\", \"히읗\", text)\n",
        "    text = re.sub(r\"ㄲ\", \"쌍기역\", text)\n",
        "    text = re.sub(r\"ㄸ\", \"쌍디귿\", text)\n",
        "    text = re.sub(r\"ㅃ\", \"쌍비읍\", text)\n",
        "    text = re.sub(r\"ㅆ\", \"쌍시옷\", text)\n",
        "    text = re.sub(r\"ㅉ\", \"쌍지읒\", text)\n",
        "    text = re.sub(r\"ㄳ\", \"기역시옷\", text)\n",
        "    text = re.sub(r\"ㄵ\", \"니은지읒\", text)\n",
        "    text = re.sub(r\"ㄶ\", \"니은히읗\", text)\n",
        "    text = re.sub(r\"ㄺ\", \"리을기역\", text)\n",
        "    text = re.sub(r\"ㄻ\", \"리을미음\", text)\n",
        "    text = re.sub(r\"ㄼ\", \"리을비읍\", text)\n",
        "    text = re.sub(r\"ㄽ\", \"리을시옷\", text)\n",
        "    text = re.sub(r\"ㄾ\", \"리을티읕\", text)\n",
        "    text = re.sub(r\"ㄿ\", \"리을피읍\", text)\n",
        "    text = re.sub(r\"ㅀ\", \"리을히읗\", text)\n",
        "    text = re.sub(r\"ㅄ\", \"비읍시옷\", text)\n",
        "    text = re.sub(r\"ㅏ\", \"아\", text)\n",
        "    text = re.sub(r\"ㅑ\", \"야\", text)\n",
        "    text = re.sub(r\"ㅓ\", \"어\", text)\n",
        "    text = re.sub(r\"ㅕ\", \"여\", text)\n",
        "    text = re.sub(r\"ㅗ\", \"오\", text)\n",
        "    text = re.sub(r\"ㅛ\", \"요\", text)\n",
        "    text = re.sub(r\"ㅜ\", \"우\", text)\n",
        "    text = re.sub(r\"ㅠ\", \"유\", text)\n",
        "    text = re.sub(r\"ㅡ\", \"으\", text)\n",
        "    text = re.sub(r\"ㅣ\", \"이\", text)\n",
        "    text = re.sub(r\"ㅐ\", \"애\", text)\n",
        "    text = re.sub(r\"ㅒ\", \"얘\", text)\n",
        "    text = re.sub(r\"ㅔ\", \"에\", text)\n",
        "    text = re.sub(r\"ㅖ\", \"예\", text)\n",
        "    text = re.sub(r\"ㅘ\", \"와\", text)\n",
        "    text = re.sub(r\"ㅙ\", \"왜\", text)\n",
        "    text = re.sub(r\"ㅚ\", \"외\", text)\n",
        "    text = re.sub(r\"ㅝ\", \"워\", text)\n",
        "    text = re.sub(r\"ㅞ\", \"웨\", text)\n",
        "    text = re.sub(r\"ㅟ\", \"위\", text)\n",
        "    text = re.sub(r\"ㅢ\", \"의\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_multiline_text(long_text):\n",
        "    texts = split_text(long_text)\n",
        "    normalized_texts = [normalize_text(text).strip() for text in texts]\n",
        "    return [text for text in normalized_texts if len(text) > 0]\n",
        "\n",
        "def synthesize(text):\n",
        "    wavs = synthesizer.tts(text, None, None)\n",
        "    return wavs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbPRQfl8z28u"
      },
      "source": [
        "### 2. 학습한 모델 불러오기\n",
        "\n",
        "Hugging face에 저장해둔 학습시킨 Glow-TTS와 HiFi-GAN 모델을 불러옵니다.\n",
        "- https://huggingface.co/soohyunn/glow-tts\n",
        "- https://huggingface.co/soohyunn/hifi-GAN\n",
        "\n",
        "<br>\n",
        "만약 다른 체크포인트에서 불러오시려면 아래 코드에서 경로를 아래와 같이 적절하게 수정합니다.\n",
        "\n",
        "```python\n",
        "synthesizer = Synthesizer(\n",
        "    \"/content/data/glow-tts/glowtts-v2-December-04-2023_10+51AM-3aa165a/best_model.pth.tar\",\n",
        "    \"/content/data/glow-tts/glowtts-v2-December-04-2023_10+51AM-3aa165a/config.json\",\n",
        "    None,\n",
        "    \"/content/data/hifi-GAN/hifigan-v2-December-04-2023_01+59PM-3aa165a/checkpoint_300000.pth.tar\",\n",
        "    \"/content/data/hifi-GAN/hifigan-v2-December-04-2023_01+59PM-3aa165a/config.json\",\n",
        "    None,\n",
        "    None,\n",
        "    False,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging face 에서 학습시킨 모델 불러오기\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/content/data\")\n",
        "os.chdir(\"/content/data\")\n",
        "!git clone https://huggingface.co/soohyunn/glow-tts\n",
        "!git clone https://huggingface.co/soohyunn/hifi-GAN"
      ],
      "metadata": {
        "id": "f3ADseOZWkGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "stat_path = ['/content/data/glow-tts/scale_stats_new.npy', '/content/data/hifi-GAN/scale_stats_new.npy']\n",
        "i = 0\n",
        "\n",
        "for file_path in roots:\n",
        "  with open(file_path, 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  data[\"audio\"][\"stats_path\"] = stat_path[i]\n",
        "\n",
        "  with open(file_path, 'w', encoding='utf-8') as file:\n",
        "      json.dump(data, file, indent=\"\\t\")\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "6BplCfV9r5YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwROk8zUHgUn"
      },
      "outputs": [],
      "source": [
        "# 체크포인트에서 모델 불러와서 음성합성 준비\n",
        "synthesizer = Synthesizer(\n",
        "    \"/content/data/glow-tts/glowtts-v2-December-04-2023_10+51AM-3aa165a/checkpoint_70000.pth.tar\",\n",
        "    \"/content/data/glow-tts/glowtts-v2-December-04-2023_10+51AM-3aa165a/config.json\",\n",
        "    None,\n",
        "    \"/content/data/hifi-GAN/hifigan-v2-December-04-2023_01+59PM-3aa165a/checkpoint_400000.pth.tar\",\n",
        "    \"/content/data/hifi-GAN/hifigan-v2-December-04-2023_01+59PM-3aa165a/config.json\",\n",
        "    None,\n",
        "    None,\n",
        "    False,\n",
        ")\n",
        "symbols = synthesizer.tts_config.characters.characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmjT_BrV0XYD"
      },
      "source": [
        "### 3. 음성 합성\n",
        "\n",
        "여기서는 실제 음성 합성을 수행합니다.\n",
        "\n",
        "위의 T2T를 통해 나온 경상도 방언 텍스트 `dialect_text`를 TTS의 input text로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSnF1D48F1tx"
      },
      "outputs": [],
      "source": [
        "for text in normalize_multiline_text(dialect_text):\n",
        "    wav = synthesizer.tts(text, None, None)\n",
        "    IPython.display.display(IPython.display.Audio(wav, rate=22050))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}